<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="FlowIBR: Leveraging Pre-Training for Efficient Neural Image-Based Rendering of Dynamic Scenes">
  <meta name="keywords" content="novel view synthesis, image based rendering, dynamic scenes, scene flow">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>FlowIBR</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">FlowIBR: Leveraging Pre-Training for Efficient Neural Image-Based Rendering of Dynamic Scenes</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <span class="author-block">
                <a href="https://www.kth.se/profile/busching?l=en">Marcel Büsching</a><sup>1</sup>,
              </span>
              <a href="https://www.chalmers.se/en/persons/bjosef/">Josef Bengtson</a><sup>2</sup>, </span>
              <span class="author-block">
                <a href="https://www.chalmers.se/personer/davini/">David Nilsson</a><sup>2</sup>, </span>
            <span class="author-block">
              <a href="https://www.kth.se/profile/celle">Mårten Björkman</a><sup>1</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>KTH Royal Institute of Technology &nbsp; </span>
            <span class="author-block"><sup>2</sup>Chalmers University of Technology  &nbsp; </span>
          </div>
        
          <h1 style="font-size:24px;font-weight:bold">Accepted to <a href="https://sites.google.com/view/ecv24/home">CVPRW ECV24</a></h1>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="http://arxiv.org/abs/2309.05418"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code coming soon</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="ADD ME"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We introduce FlowIBR, a novel approach for efficient monocular novel view synthesis of dynamic scenes. 
            Existing techniques already show impressive rendering quality but tend to focus on optimization within a single scene without leveraging prior knowledge, resulting in long optimization times per scene. 
            <p>
              FlowIBR circumvents this limitation by integrating a neural image-based rendering method, pre-trained on a large corpus of widely available static scenes, with a per-scene optimized scene flow field.
              Utilizing this flow field, we bend the camera rays to counteract the scene dynamics, thereby presenting the dynamic scene as if it were static to the rendering network.
              The proposed method reduces per-scene optimization time by an order of magnitude, achieving comparable rendering quality to existing methods -- all on a single consumer-grade GPU.</p>
              </div>
      </div>
    </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-max-desktop">
        <h2 class="title is-3">Method</h2>
        <image src="static/images/overview.svg" />
        <div class="content has-text-justified">
            <b>a)</b> An image at an arbitrary position (orange camera) is synthesised based on existing observations (black camera), collected at different times. 
            <b>Problem:</b> Due to the movement of the skater, the skater is not on the epipolar line of the camera ray, which is necessary for image-based rendering.
            <b>b)</b> We model scene motion using per-scene learned scene flow.
            <b>c)</b> Scene flow is used to compensate for the motion by adjusting the camera ray. 
            <b>d)</b> GNT, a pre-trained neural IBR method for static scenes is used for image synthesis.
          </div>
      </div>
    </div>
</section>
<section class="hero teaser">
  <div class="container is-max-desktop">      
    <h2 class="columns is-centered has-text-centered title is-3">Video results</h2>
    <div class="hero-body">
      <center>
      <video id="teaser" autoplay muted loop playsinline controls width="99%">
        <source src="static/videos/video-compr.mp4" type="video/mp4">
      </video>
      </center>
      <!-- <h3 class="subtitle has-text-centered">
        <font size="-1">
        <span class="dnerf"> </span> 
        State-of-the-art monocular dynamic view synthesis methods struggle to render high-quality views from long videos featuring uncontrolled camera paths and complex scene motion. We present a new approach that addresses these limitations, illustrated above via an application to 3D stabilization, where we apply our approach and prior methods to a shaky, 30-second video, and compare novel views rendered along a smoothed camera path. 
        </font>
      </h3> -->
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{
  buesching2023flowibr,
  title         = {FlowIBR: Leveraging Pre-Training for Efficient Neural Image-Based Rendering of Dynamic Scenes}, 
  author        = {Marcel Büsching, Josef Bengtson, David Nilsson, Mårten Björkman},
  year          = {2023},
  eprint        = {2309.05418},
  archivePrefix = {arXiv},
}</code></pre>
  </div>
</section>
<footer class="footer">
  <div align="center" class="container">
    <div class="columns is-centered">
        <div class="content">
          This website is borrowed from <a href="https://github.com/3d-moments/3d-moments.github.io">3D Moments</a> and <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
        </div>
      </div>
    </div>
</footer>

</body>
</html>
